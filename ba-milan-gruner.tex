\documentclass[
        a4paper,     % Format A4
        titlepage,   % mit Titelseite
        oneside,     % twoside => zweiseitig
        parskip      % mit Durchschuss
                     % (= Abstand zwischen Absätzen, statt Einrückung)
        ]{scrartcl}  % KOMA-Script Grundklasse     texdoc scrguide

%\usepackage{ngerman}             % Deutsche Sprache, neue RS   texdoc germdoc (?)
\usepackage[T1]{fontenc}          % Schriftkodierung mit Umlauten
\usepackage{textcomp,amsmath}     % Mathezeichen etc.
\usepackage{graphicx}             % Graphiken einbinden
\usepackage{hyperref}             % Hyperlinks fuer TOC, Glossar, etc.
%\usepackage{beramono}             % Font for code listings (?)
\usepackage{listings}             % Code snippets
\usepackage{xcolor}               % Custom colors in code listings

% Listing color definitions
\definecolor{numbercolor}{rgb}{0.5,0.5,0.5}
\definecolor{keywordcolor}{rgb}{0.7,0.4,0.0}
\definecolor{commentcolor}{rgb}{0,0.6,0}
\definecolor{stringcolor}{rgb}{0.7,0.0,0.2}

% Scala code snippet styling
\lstdefinestyle{scalaStyle}{
  language=scala,
  morekeywords={String, Long, UUID, Map, Nil},
  otherkeywords={!,<=,>=,=>,!=},
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numbersep=1pt,
  numberstyle=\small\ttfamily\color{numbercolor},
  keywordstyle=\color{keywordcolor},
  commentstyle=\color{commentcolor},
  stringstyle=\color{stringcolor},
  frame=none,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}

% bibtex
\usepackage{url}
\bibliographystyle{plaindin}      % BibTeX Styles nach Norm DIN 1505

\include{glossaries}

\titlehead{
  \includegraphics{hpi_logo_cmyk_wb_sl2}
}
\subject{Bachelorarbeit}
\title{
  Graph Exploration and Analysis on Business Graphs
  \\ \bigskip
  \large{Graphexploration und -analyse auf Unternehmensgraphen}
}
\author{Milan Gruner\\{\small{\url{milangruner@gmail.com}}}}
\date{Eingereicht am <TBD>}
\publishers{
  Fachgebiet Informationssysteme \\
  Betreuung: Prof. Dr. Felix Naumann, Michael Loster, Toni Gruetze
}

\pagestyle{headings}    % Seitenstil mit Kapitelüberschriften in der Kopfzeile

\begin{document}

  \maketitle    %  Titelseite erzeugen
  \clearpage % neue Seite (war \cleardoublepage)

  \section*{Abstract}
  { \large
    Costructing a graph made up of thousands of businesses may be hard, but actually making sense of it is a lot harder.
    With huge amounts of data potentially being integrated into the data lake every day, automatic methods for finding interesting spots in the graph are needed.
    This paper discusses different approaches that can be taken to extract useful knowledge from such a graph.
  }

  \clearpage
  \tableofcontents
  \pagebreak
  %\listoffigures
  %\listoftables

	%\glsaddall
  %\printglossary
  %\printglossary[type=\acronymtype
  %\Gls{business} \\
  %\gls{lambda} \\
  %\gls{bgn} \\
  %\cite{bloom70space-time}

  \section{Introduction}
    This paper describes the basic approaches that were taken for analyzing the
    graphs generated as a part of the Ingestion bachelor project at HPI Potsdam.
    It deals with the experiments I performed to evaluate certain algorithms and approaches to summarize
    automatically generated graphs in the ball park of tens of millions of nodes in
    a large and mostly unconnected and sparse graph.
    This graph contains businesses, places and persons and can be augmented freely by adding
    additional data sources (or manual data input) of your own. The data is visualized and controllable from the
    Curation interface, which was a project developed alongside the Ingestion pipeline.
    Curation generates graphs on-the-fly that contain the statistics data that was written
    as a side effect of the numerous Spark jobs in the Ingestion pipeline and gives
    a lot of insight into what's going on in the "data lake".
    All of the experiments contained in this paper were conducted using either
    the Curation interface, the Apache Zeppelin interactive web shell or the Spark shell.

    \subsection{Technologies used in this paper}
    Most of the topics in this paper deal with graph-related topics, so the GraphFrames framework
    (in Scala) will mostly be used to describe the algorithms that were employed,
    but it can be freely seen as functional pseudo code and adapted to other popular
    graph processing frameworks (such as GraphX, Giraph etc.) % TODO more examples
    For UI-related or user-oriented algorithms, JavaScript (or JSX, so partly using React/ Redux etc.)
    is the listing language, as visual problems are more easily expressed in this
    tree-oriented but still functional style (and also because it's used in Curation). % TODO remove?
    Whereever possible, ES2016 Syntax is used for the brevity and the similarity
    to the functional approach of Scala and the rest of the example code.

    Also one of the purposes of the Ingestion and Curation projects was to enable
    the execution of modern text mining, deduplication and parallel data analysis techniques
    to the german business landscape. This means that some of the examples will be
    in German (but they will be translated if needed for understanding the technique).

    \subsection{Usage Scenarios of Ingestion, Curation and GraphXplore}
    The process of classic risk analysis which is still majorly employed in today's banks is mostly
    a model-oriented process that heavily relies on speculation or predictions.\\
    The techniques that Ingestion, Curation and this paper offer are universally
    usable to analyze huge amounts of structured or unstructured data
    in the form of WikiData, JSON, CSV or similar data as well as newspaper articles,
    messages (eMail, instant or otherwise) or similar data sources.\\
    The results of this can either be viewed as a graph or a relational model
    using the conversion algorithms briefly discussed later in this paper.
    Because this project consists of automatic processes
    that control the flow, categorization and extraction
    of all the data in the data lake, it may be effectively deployed in a bank
    or similar institution as a data collection, curation and analysis software stack.\\
    It is meant to be heavily extensible and allows the simple integration of new data sources % TODO really possible in the end?
    as well as algorithms and data structures in general. The pipeline itself is very modular
    and is composed of many stand-alone Spark jobs that mostly read and write from or to the Cassandra database.
    \pagebreak

  \section{Data structures for business entities}
    \subsection{The $Subject$ data structure}
    The central data structure that exists inside the datalake is called $subject$.
    It contains all the final businesses, industry sectors, countries or persons
    that are the result of the computations in the Ingestion project.\\
    This is what it looks like in Scala:
    \begin{lstlisting}[style=scalaStyle,caption=Subject]
    case class Subject(
    	var id: UUID = UUID.randomUUID(),
    	var master: UUID,
    	var datasource: String,
    	var name: Option[String] = None,
    	var aliases: List[String] = Nil,
    	var category: Option[String] = None,
    	var properties: Map[String, List[String]] = Map(),
    	var relations: Map[UUID, Map[String, String]] = Map(),

    	var master_history: List[Version] = Nil,
    	var name_history: List[Version] = Nil,
    	var aliases_history: List[Version] = Nil,
    	var category_history: List[Version] = Nil,
    	var properties_history: Map[String, List[Version]] = Map(),
    	var relations_history: Map[UUID, Map[String, List[Version]]] = Map()
    )
    \end{lstlisting}

    The first three attributes ($id$, $master$, $datasource$) constitute the
    primary key in the Cassandra databases.
    The partitioning key that is used to decide on which Cassandra instance to store
    a given $Subject$ entry is the $master$ ID, which makes sure that a master node
    and all its data source slave nodes are stored on the same machine.
    $id$ and $datasource$ are used as clustering keys, which control the sorting
    inside the data partitions produced by database queries. They are also easily
    queryable when specific IDs or data source entries for a given master ID are
    needed.

    When loading Cassandra data into a Spark job, simple fields (e.g. Strings)
    need to be wrapped in an Option in case that the field contains no value
    (otherwise there would be an error), which isn't necessary for the fields
    containing Maps or Lists, which would just contain an empty data structure
    in that case.

    The $properties$ field contains all the extracted data from the various
    data source extraction and normalization steps in the Ingestion pipeline.
    It is structured in a way to make storing arbitrary data possible, by allowing
    to store a List of Strings for each property key. This way, a list of
    eMail addresses can be stored as well as a single postal code and they can
    be treated the same way.

    The $relations$ field is structured in a two-dimensional map. The first level
    is addressed by the target $subject$ ID, while the second level's keys are the
    specific relation type. The value that is returned when the field is queried
    by the target ID and relation type is the relation attribute, which is mostly
    used to store a confidence measure (a value from 0 to 1) or the count of the
    relation type between the current $subject$ and the target.

    \subsection{A versioning scheme that stands the test of time}
    Versioning is needed for the automatic system that includes various pipelines
    that write to lots of different tables in Cassandra during their runtime,
    but at the end it is expected that all data will flow back into the central
    'data lake' that is the $subject$ table, described earlier in this chapter.
    This means that fully automated processes may and will write millions of
    data entries to the datalake at an hourly basis (or even more frequent),
    so if any breaking changes are introduced by these, they need to be rerolled quickly.
    This means that each version isn't saved as a diff like in version control systems
    (e.g. git), but rather every single version has a copy of all the attributes,
    a timestamp and the name and version of the program that modified it.
    As our whole pipeline is made up of individual small Spark jobs that write
    mostly into their own Cassandra tables, the error in the data can be quickly
    located and traced to the corresponding source code.

    Because our data domain included events, attributes and relations that had
    temporal validity parameters attached to them, we had to realize a versioning
    scheme that was compatible with the column-family oriented storage of Cassandra.
    As everything is distributed across the cluster here, a diff-oriented versioning
    would be hard to keep up, so our team decided on a deeply structured deep copy
    backup system for our versions.

    The version data structures are stores for each attribute and each individual
    key of the subject entries, which makes restoring individual columns or single
    data entries efficient and easily parallelizable.

    The version data structure that was used is structured like this:
    \begin{lstlisting}[style=scalaStyle,caption=Version]
    case class Version(
    	version: UUID = UUIDs.timeBased(),
    	var program: String,
    	var value: List[String] = Nil,
    	var validity: Map[String, String] = Map(),
    	var datasources: List[String] = Nil,
    	var timestamp: Date = new Date()
    )
    \end{lstlisting}

    It is stored inside the collections of the $history$ fields of the $Subject$
    data structure and contains useful meta information for restoring old versions,
    modeling temporal validity (attributes only being valid for a certain timespan)
    or filtering out data from certain data sources. They also contain the Spark
    program's name that created the relevant changes and the time they were made.

    Additionally, the $version$ table contains a list of all changes to the datalake,
    making it traceable which programs were ran at what time, which table they wrote
    to and what data sources they processed. Using this, the Curation interface
    can display the annotated version list, and run Spark jobs that restore the
    whole data lake to a previous version or compute the changes that a version
    made in the $subject$ table.
    \pagebreak

  \section{Using Apache Spark and GraphFrames for Graph Analysis}
  This chapter will briefly discuss the reasons for using Spark's GraphX API
  alongside the Apache Cassandra database. Then the transformations that are
  necessary to extract GraphFrames-compatible data from the data lake will be detailed.
  The following chapters will build on the techniques described in this one, so
  that only the relevant graph processing sections need to be shown later on.

    \subsection{Why Spark, Cassandra and GraphFrames?}
    The subject table contains millions of entries. In order to efficiently extract
    a graph and calculate the results of complex graph algorithms, a parallel processing
    framework like Apache Spark is a really important measure to take into consideration when runtimes
    of multiple hours (or even days) aren't desired. Cassandra is a database that
    is up to the challenge of delivering the input for and storing the results of
    these computations without much hassle (like serializing to CSV or Parquet files would involve).
    It is also well integrated into the Spark API and can take care of complex nested
    data structures like e.g. the properties and relations attributes of the $Subject$
    case class.

    TODO continue

    \subsection{Constructing a GraphFrame from subject data}
    The core class of the GraphFrame API is the class GraphFrame itself, which
    is basically a combination of a vertex and relation dataframe as well as a
    lot of operations which can be performed on these. It is a useful tool for
    easily handling the graph data structure and interacting with it in a functional
    manner whilst still being able to interact with the relational API of Spark's dataframes.

    The case class entries that are queried from Cassandra are transformed into vertex
    and relation data using the Spark RDD API. The results are then translated into
    data frames, which GraphFrames expects when creating a new graph instance.

    The $extractGraph$ function from $GraphExtractor$ takes care of this process:

    \begin{lstlisting}[style=scalaStyle,caption=extractGraph in $GraphExtractor$]
    def extractGraph(subjects: RDD[Subject], sc: SparkContext): GraphFrame = {
  		val spark = SparkSession
  			.builder()
  			.appName(appName)
  			.config(sc.getConf)
  			.getOrCreate()
  		import spark.implicits._

  		val subjectVertices = subjects
  			.map(subject => (subject.id.toString, subject.name))
  			.toDF("id", "name")
  		val subjectRelations = subjects.flatMap(subject => {
  			subject.masterRelations.flatMap { case (id, relTypes) =>
  				relTypes.map { case (relType, value) =>
  					(subject.id.toString, id.toString, relType, value)
  				}
  			}
  		}).toDF("src", "dst", "relationship", "value")

  		GraphFrame(subjectVertices, subjectRelations)
  	}
    \end{lstlisting}

    A spark session has to be manually created in the beginning because the implicit .toDF() method
    needed for transforming the RDD into a GraphFrame is contained in either a
    Spark SQL Context or a session object, the former being the deprecated way of gaining access to this method.

    The three nested $map$ statements make every relation type to every target subject
    contained into a new tuple in the resulting RDD. Because of the outer $flatMap$
    calls, the algorithm gets rid off all of the nested structure of the $relations$
    attribute, adding the source node to every given triple of the target, relation type, and relation value.
    It then names them in the scheme that the $GraphFrame$ constructor expects.
    \pagebreak

  \section{Approaches for subgraph extraction}
  This chapter deals with various graph processing jobs developed using the
  GraphFrames framework that extract useful subsets of the full graph in the data lake.
  These are some examples of simple graph processing that lay the cornerstone
  for the remaining graph evaluation techniques discussed in later chapters.

  \subsection{Finding company groups in the business graph}
  As a first experiment with real-world applications, company group extraction
  was a good starting algorithm that could be adapted from the connected component
  tagging which was already present in the GraphFrames API.
  Here, all businesses that were connected via "owns" or "owned by"
  relations were considered as a company group.

  Before the ConnectedComponets algorithm is run, all ownership-related company
  relations are extracted from the $Subject$ graph. This is done by whitelisting
  all relations (in $ownershipRelations$) that concern this particular subgraph,
  e.g. "owns" and "ownedBy".
  Then, after executing the search, all entries are grouped by the ID of the
  associated component and their names and UUIDs are saved to Cassandra
  as an entry of the $graphs$ table. The following code snippet describes how
  this process is expressed using GraphFrames:

  \begin{lstlisting}[style=scalaStyle,caption=Company Group Extraction]
  val ownershipEdges = graph.edges.filter((edge) =>
    ownershipRelations.contains(edge.getAs[String]("relationship")))
  val ownershipGraph = GraphFrame(graph.vertices, ownershipEdges)

  ownershipGraph.connectedComponents.run()
		.select("id", "component", "name")
		.rdd
		.map(row => (
			row.getAs[Long]("component"),
			UUID.fromString(row.getAs[String]("id")),
			row.getAs[String]("name")))
		.groupBy(_._1) // component
		.map(_._2.map(t => (t._2, t._3)).toList) // extract id and name
		.filter(_.length >= minComponentSize)
		.map(idNameTuples => ResultGraph(
			outputGraphType,
			idNameTuples.map(_._1),
			idNameTuples.filter(_._2 != null).map(_._2)))
  \end{lstlisting}

  The result is a list of businesses like the following:

  Oracle America, Hyperion Solutions, Sun Microsystems, Micros Systems, Oracle Financial Services Software, Oracle, PeopleSoft, Acme Packet, Art Technology Group

  The corresponding IDs are also stored alongside the names and the graph type (here: "CompanyGroup"). This makes it easy to analyze
  the results without always joining all subject's names and IDs to this dataset. It also facilitates displaying
  a graph list in the Curation interface, because only one table's content needs to be fetched for it.
  \pagebreak

  \section{Graph Simplification using PageRank and Thresholding}
  The PageRank algorithm by Page et al. \cite{pagerank1999} was originally intended
  to compute the likelihood that a user might arrive at a web page
  in a network of interconnected documents. It can however be adapted to most
  graph structures and also finds an application when dealing with business graphs,
  as the navigation between businesses over their outgoing relations is quite
  alike to the navigation between web pages over their hyper links.
  Therefore, the vertex output of the algorithm can be interpreted as the
  probability that a user might arrive at a certain business when freely
  navigating the graph, starting at any connected node.
  The relation output can be seen as the likelihood that the user
  travels along this relation to the next company.

  \subsection{Using PageRank to find relevant companies}
  Fortunately, PageRank is already implemented in the GraphFrames library.
  There are two different possibilities to call it on a given graph: it can either
  be executed for a given number of iterations, or it can be run until convergence
  is reached within a certain tolerance interval to keep the algorithm from running endlessly.
  Experience has shown that the former is quite a lot faster, but the number of iterations
  has to be manually adapted to the graph size and its degree of interconnectedness.
  Otherwise the results differ too much from the latter implementation.
  Here, the only parameter that has to be controlled is the tolerance threshold,
  which is independent of the input graph because it only depends on the accuracy that is desired.

  After the PageRank algorithm is executed, the results still have to be interpreted,
  meaning that the nodes have to be separated into important and unimportant groups.
  This can be done using thresholding techniques and can be parallelized
  over the connected components of the business graph.

  First, the individual $pagerank$ values assigned to every node have to be normalized
  over the whole connected component, which is done by finding the maximum of this
  attribute in the component and dividing every node's attribute by it.
  Then, a threshold value has to be picked for the component. This can either be done
  using an absolute threshold that has to be determined for the whole graph, or
  using an adaptive thresholding technique like Otsu's method \cite{otsu1979threshold},
  which is normally used for image thresholding but can be applied here as well.

  It works by creating a histogram of the absolute frequency of
  each $pagerank$ value present in the component, which are grouped into bins.
  Then, the values are iteratively separated into two classes while trying to
  minimize the variance inside these classes.\\
  In the end, a threshold is picked that can be used to decide whether or not to
  include a node in the important group based on its $pagerank$ value.

  The same can be done for the relations in the current connected component.
  By only showing the edges that the user is likely to use, the graph can be greatly
  simplified. This makes use of the weights the PageRank algorithm assigns to the
  relations present in the graph.

  \subsection{Finding connections between relevant companies}

  \pagebreak

  \section{Motif Search}
    \subsection{Pattern types and their applications}
    \subsection{Related work}
    \pagebreak

  \section{Pattern Analysis}
    \subsection{User-aided approaches for Pattern Categorization}
    \subsection{Pattern importance measures}
    \subsection{Machine Learning Models for analyzing user feedback}
    \subsection{Related work}
    \pagebreak

  \section{Benchmarks and Experiments}
    \subsection{GraphX vs. Plain RDDs}
    \subsection{Design decisions and trade-offs}
    \subsection{Technical challenges}
    \pagebreak

  \section{Conclusion and outlook}

  \clearpage
  \bibliography{references}

\end{document}
